{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "### Overfitting a Decision Tree\n",
    "This bug was found when Katie was trying to make an overfit decision tree to use as an example in the decision tree mini-project. A decision tree is classically an algorithm that can be easy to overfit; one of the easiest ways to get an overfit decision tree is to use a small training set and lots of features.\n",
    "If a decision tree is overfit, would you expect the accuracy on a test set to be very high or pretty low?\n",
    "* Low!\n",
    "\n",
    "If a decision tree is overfit, would you expect high or low accuracy on the training set?\n",
    "* High!\n",
    "\n",
    "### Number of Features and Overfitting\n",
    "A classic way to overfit an algorithm is by using lots of features and not a lot of training data. You can find the starter code in feature_selection/find_signature.py. Get a decision tree up and training on the training data, and print out the accuracy. How many training points are there, according to the starter code?\n",
    "\n",
    "How many training points are there, according to the starter code?\n",
    "* 150\n",
    "\n",
    "### Accuracy of Your Overfit Decision Tree\n",
    "What’s the accuracy of the decision tree you just made? (Remember, we're setting up our decision tree to overfit -- ideally, we want to see the test accuracy as relatively low.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"your_word_data.pkl\" \n",
    "authors_file = \"your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"rb\"))\n",
    "authors = pickle.load( open(authors_file, \"rb\") )\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "### your code goes here\n",
    "from sklearn import tree\n",
    "\n",
    "# Create classifier\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 40)\n",
    "\n",
    "# Fit the classifier on the training features and labels\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# Make prediction - Store predictions in a list named pred\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Calculate the accuracy on the test data\n",
    "print(\"Accuracy: {}\".format(clf.score(features_test, labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Most Powerful Features\n",
    "Take your (overfit) decision tree and use the feature_importances_ attribute to get a list of the relative importance of all the features being used. We suggest iterating through this list (it’s long, since this is text data) and only printing out the feature importance if it’s above some threshold (say, 0.2--remember, if all words were equally important, each one would give an importance of far less than 0.01). What’s the importance of the most important feature? What is the number of this feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "\n",
    "cnt = 0\n",
    "for importance in importances:\n",
    "    if importance > 0.2:\n",
    "        print(\"Importance of the most important feature: {}\".format(importance))\n",
    "        print(\"Number of this feature: {}\".format(cnt))\n",
    "    cnt += 1\n",
    "\n",
    "print()\n",
    "# Alternative solution\n",
    "for index, importance in enumerate(importances):\n",
    "    if importance > 0.2:\n",
    "        print(\"Importance of the most important feature: {}\".format(importance)) \n",
    "        print(\"Number of this feature: {}\".format(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TfIdf to Get the Most Important Word\n",
    "In order to figure out what words are causing the problem, you need to go back to the TfIdf and use the feature numbers that you obtained in the previous part of the mini-project to get the associated words. You can return a list of all the words in the TfIdf by calling get_feature_names() on it; pull out the word that’s causing most of the discrimination of the decision tree. What is it? Does it make sense as a word that’s uniquely tied to either Chris Germany or Sara Shackleton, a signature of sorts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names()\n",
    "print(\"Word: {}\".format(words[33614]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Even though our training data is limited, we still have a word that is highly indicative of author.\n",
    "\n",
    "### Remove, Repeat\n",
    "This word seems like an outlier in a certain sense, so let’s remove it and refit. Go back to text_learning/vectorize_text.py, and remove this word from the emails using the same method you used to remove “sara”, “chris”, etc. Rerun vectorize_text.py, and once that finishes, rerun find_signature.py. Any other outliers pop up? What word is it? Seem like a signature-type word? (Define an outlier as a feature with importance >0.2, as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        # words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        text_string = text_string.split()\n",
    "        for text in text_string:\n",
    "            words = words + stemmer.stem(text) + \" \"\n",
    "\n",
    "    return words\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list\n",
    "temp_counter = 0\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        # temp_counter += 1\n",
    "        if temp_counter < 200:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            text = parseOutText(email)\n",
    "\n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            ### remove sshacklensf\n",
    "            unwanted = [\"sara\", \"shackleton\", \"chris\", \"germani\", \"sshacklensf\"]\n",
    "            for words in unwanted:\n",
    "                text = text.replace(words, \"\")\n",
    "\n",
    "            ### append the text to word_data\n",
    "            word_data.append(text)\n",
    "\n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            if name == \"sara\":\n",
    "                from_data.append(0)\n",
    "            else:\n",
    "                from_data.append(1)\n",
    "\n",
    "            email.close()\n",
    "\n",
    "print(\"emails processed\")\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data_new.pkl\", \"wb\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors_new.pkl\", \"wb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_file = \"your_word_data_new.pkl\" \n",
    "authors_file = \"your_email_authors_new.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"rb\"))\n",
    "authors = pickle.load( open(authors_file, \"rb\") )\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "# Create classifier\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 40)\n",
    "\n",
    "# Fit the classifier on the training features and labels\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# Make prediction - Store predictions in a list named pred\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Calculate the accuracy on the test data\n",
    "print(\"Accuracy: {}\".format(clf.score(features_test, labels_test)))\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "for index, importance in enumerate(importances):\n",
    "    if importance > 0.2:\n",
    "        print(\"Importance of the most important feature: {}\".format(importance)) \n",
    "        print(\"Number of this feature: {}\".format(index))\n",
    "        print(\"Word: {}\".format(words[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Important Features Again\n",
    "Update vectorize_test.py one more time, and rerun. Then run find_signature.py again. Any other important features (importance>0.2) arise? How many? Do any of them look like “signature words”, or are they more “email content” words, that look like they legitimately come from the text of the messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list\n",
    "temp_counter = 0\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        # temp_counter += 1\n",
    "        if temp_counter < 200:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            text = parseOutText(email)\n",
    "\n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            ### remove sshacklensf\n",
    "            unwanted = [\"sara\", \"shackleton\", \"chris\", \"germani\", \"sshacklensf\", \"cgermannsf\"]\n",
    "            for words in unwanted:\n",
    "                text = text.replace(words, \"\")\n",
    "\n",
    "            ### append the text to word_data\n",
    "            word_data.append(text)\n",
    "\n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            if name == \"sara\":\n",
    "                from_data.append(0)\n",
    "            else:\n",
    "                from_data.append(1)\n",
    "\n",
    "            email.close()\n",
    "\n",
    "print(\"emails processed\")\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data_new2.pkl\", \"wb\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors_new2.pkl\", \"wb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_file = \"your_word_data_new2.pkl\" \n",
    "authors_file = \"your_email_authors_new2.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"rb\"))\n",
    "authors = pickle.load( open(authors_file, \"rb\") )\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "# Create classifier\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 40)\n",
    "\n",
    "# Fit the classifier on the training features and labels\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# Make prediction - Store predictions in a list named pred\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Calculate the accuracy on the test data\n",
    "print(\"Accuracy: {}\".format(clf.score(features_test, labels_test)))\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "for index, importance in enumerate(importances):\n",
    "    if importance > 0.2:\n",
    "        print(\"Importance of the most important feature: {}\".format(importance)) \n",
    "        print(\"Number of this feature: {}\".format(index))\n",
    "        print(\"Word: {}\".format(words[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is one more important word (\"houectect\"). It doesn't look like an obvious signature word so let's keep moving without removing it.\n",
    "\n",
    "### Accuracy of the Overfit Tree\n",
    "What’s the accuracy of the decision tree now? We've removed two \"signature words\", so it will be more difficult for the algorithm to fit to our limited training set without overfitting. Remember, the whole point was to see if we could get the algorithm to overfit--a sensible result is one where the accuracy isn't that great!\n",
    "\n",
    "* 0.8122866894197952"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
